{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from haralyzer import HarParser\n",
    "from urllib.request import urlopen , unquote\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#1Sec = 1000Milisec\n",
    "inputFile = 'user.har'\n",
    "inPath = './data/'\n",
    "outPathFile = './output/'+inputFile\n",
    "\n",
    "with open(inPath + inputFile , 'r',encoding=\"utf8\") as f:\n",
    "    har_parser = HarParser(json.loads(f.read()))\n",
    "\n",
    "\n",
    "userRequestPages = [] ### User Requested Pages ###\n",
    "entries = [] ### CREATE A TIMELINE OF ALL THE ENTRIES ###\n",
    "for page in har_parser.pages:\n",
    "    userRequestPages.append((page.url, page.image_load_time,page.page_size, page.image_size))\n",
    "    #userRequestPages.append((page.url,0,0,0))\n",
    "    for entry in page.entries:\n",
    "        entries.append(entry)\n",
    "\n",
    "#User Requested pages\n",
    "userRequestPagesDF = pd.DataFrame(userRequestPages,columns=['url','loadtime','page_size','image_size'])\n",
    "userRequestPagesDF.to_csv(outPathFile+'_userRequestPages.tsv',sep='\\t',header=True,index=False)\n",
    "\n",
    "timeline = har_parser.create_asset_timeline(entries)\n",
    "webRequestURL = []\n",
    "for key, value in timeline.items():\n",
    "    for data in value:\n",
    "        webRequestURL.append((data['request']['url'],data['time'])) # ,data['request']['queryString']\n",
    "\n",
    "domain = set()\n",
    "domainURL = []\n",
    "domainDqpKeyValue = []\n",
    "df = pd.DataFrame(webRequestURL,columns=['url','time'])\n",
    "df = df.groupby('url')['time'].sum().reset_index()\n",
    "\n",
    "\n",
    "def cleanDqp(dqp):\n",
    "    return re.sub('=+','=',re.sub('&+','&',dqp)).strip()\n",
    "\n",
    "def keyValueProcessing(domainDqpKeyValue,dqp,fkey=''):\n",
    "    if fkey != '': fkey = fkey + \"_\"\n",
    "    dqp = cleanDqp(dqp)\n",
    "    dqpsplit = dqp.split('&')\n",
    "    for keyValue in dqpsplit:\n",
    "\n",
    "        keyValue = keyValue.strip()\n",
    "        keyValueList = keyValue.split('=')\n",
    "\n",
    "        try:\n",
    "            if len(keyValueList) == 2:\n",
    "                key, value = keyValueList[0], unquote(unquote(keyValueList[1]))\n",
    "                domainDqpKeyValue.append((domainPart,baseurl,fkey+key,value))\n",
    "                if re.search('&',value): keyValueProcessing(domainDqpKeyValue,value,key)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "for index,row in df[['url','time']].drop_duplicates().iterrows():\n",
    "    url = row['url']\n",
    "    domainPart = url.split('/')[2]\n",
    "    if re.search('\\?',url):\n",
    "        urlSplit = url.split('?')\n",
    "        baseurl,dqp = urlSplit[0],urlSplit[1]\n",
    "        if re.search('&',dqp):\n",
    "            keyValueProcessing(domainDqpKeyValue,dqp)\n",
    "            url = baseurl\n",
    "        elif(re.search('=',dqp)):\n",
    "                keyValueList = dqp.split('=')\n",
    "                try:\n",
    "                    key, value = keyValueList[0], unquote(unquote(keyValueList[1])) #.decode('utf8')\n",
    "                    domainDqpKeyValue.append((domainPart,baseurl,key,value))\n",
    "                    if re.search('&',value): keyValueProcessing(domainDqpKeyValue,value)\n",
    "                    url = baseurl\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "    domain.add( domainPart)\n",
    "    domainURL.append((domainPart,row['url'],row['time']))\n",
    "    \n",
    "#Turning DomainURL data to a dataframe\n",
    "domainURLDF = pd.DataFrame.from_records(domainURL, index=None, exclude=None, columns = ['domain','url','time'])\n",
    "#grouping by domain\n",
    "#domainURLDF.groupby(['domain'])['time'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the Title of the website\n",
    "endDomains = ['com','net','org','co']\n",
    "\n",
    "def cleanTitle(title):\n",
    "    if re.match('error',title): title = ''\n",
    "    return re.sub('\\s+',' ',re.sub('\\n','',title)).strip()\n",
    "\n",
    "websiteTitle = []\n",
    "for website in domain:\n",
    "    title = website\n",
    "    mainDomain = subDomain = ''\n",
    "    domainPartList = website.split('.')\n",
    "    if domainPartList[-1] in endDomains:\n",
    "        mainDomain = domainPartList[-2]+'.'+ domainPartList[-1]\n",
    "        subDomain = '.'.join(domainPartList[:-2])\n",
    "    #Get Title of the main website\n",
    "    try:\n",
    "        html = urlopen('http://'+mainDomain, timeout = 3)\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        title = cleanTitle(soup.title.string)\n",
    "    except:\n",
    "        #If main website errors out then try with subdomain\n",
    "        try:\n",
    "            html = urlopen('http://'+website, timeout = 3)\n",
    "            soup = BeautifulSoup(html,'html.parser')\n",
    "            title = cleanTitle(soup.title.string)\n",
    "        except:\n",
    "            pass\n",
    "    if title == None or title == '': title = website\n",
    "    websiteTitle.append((title,website,mainDomain,subDomain))\n",
    "    \n",
    "websiteTitleDF = pd.DataFrame.from_records(websiteTitle, index=None, exclude=None, columns = ['title','domain','maindomain','subdomain'])\n",
    "# websiteTitleDF.to_csv(outPathFile+'_domainTitles.tsv',sep='\\t',header=True,index=False)\n",
    "\n",
    "def get_stats(group):\n",
    "    return {'min': group.min(), 'max': group.max(), 'Calls': group.count(), 'mean': group.mean(), 'Time': group.sum()}\n",
    "\n",
    "domainCallsorig = domainURLDF['time'].groupby(domainURLDF['domain']).apply(get_stats).unstack().sort_values('Time',ascending=False).reset_index()\n",
    "domainCallsorig = domainCallsorig.merge(websiteTitleDF,how='inner' ,left_on =['domain'],right_on=['domain'])[['domain', 'Calls', 'Time', 'max', 'mean', 'min', 'title', 'maindomain', 'subdomain']]\n",
    "domainCallsorig[['title','domain','maindomain', 'subdomain','Calls', 'Time', 'max', 'mean', 'min']].to_csv(outPathFile+'_summary.tsv',sep='\\t',header=True,index=False)\n",
    "#Saving the domain ,baseURL, Key , Value data\n",
    "pd.DataFrame(domainDqpKeyValue,columns=['domain','url','key','value']).to_csv(outPathFile+'_domainKeyValue.tsv',sep='\\t',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#D3js charting data\n",
    "def label_domains(row):\n",
    "    label = row['domain']\n",
    "    if row['Calls'] == 1: label = 'OneCallDomain'\n",
    "    if row['Calls'] == 2: label = 'TwoCallDomain'\n",
    "    return label\n",
    "\n",
    "domainCallsorig['repDomain'] = domainCallsorig.apply(label_domains, axis=1)\n",
    "domainCallsDataforViz = domainCallsorig.groupby(domainCallsorig['repDomain']).sum().reset_index().rename(index=str,columns={\"repDomain\":\"id\",\"Calls\":\"value\"})[['id','value']]\n",
    "domainCallsDataforViz['id'] = domainCallsDataforViz['id'].str.replace('.','*')\n",
    "domainCallsDataforViz.to_csv(outPathFile+'_bubble.csv',sep=',',header=True,index=False)\n",
    "domainCallsDataforViz.columns = ['letter','frequency']\n",
    "domainCallsDataforViz.to_csv(outPathFile+'_bar.tsv',sep='\\t',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
